# Release Year Prediction
## Estimate release_year from the description text and cast â€” this is noisy but can test temporal signal detection.

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

# Pull Data & Display Head
df = pd.read_csv("./netflix_titles.csv")
print(df.head(5))

df['text'] = df['type'].fillna("") + df['title'].fillna("") + df['cast'].fillna("")

# Drop Rows with out 'release_year' value
df = df.dropna(subset='release_year')

# Drop Years with only 1 Sample in Data
count = df['release_year'].value_counts()
df = df[df['release_year'].isin(count[count > 1].index)]

# Encode Labels
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['release_year'])
num_classes = len(label_encoder.classes_)
print("Classes:", label_encoder.classes_)

# Create Test/Train Data Sets
X_train, X_test, y_train, y_test = train_test_split(
  df['text'], df['label'],
  test_size=0.2,
  random_state=42,
  stratify=df['label']
)

# Class Weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))

# Tokenization (required for Neural Networks)
MAX_VOCAB = 20000   # 20,000 more frequent words
MAX_LEN = 200       # 200 tokens is expexted length (required)
tokenizer = tf.keras.preprocessing.text(num_words=MAX_VOCAB, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# Padding (required for Neural Networks)
X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(
  tokenizer.texts_to_sequences(X_train), maxlen=MAX_LEN, padding='post', truncating='post'
)
X_test_pad  = tf.keras.preprocessing.sequence.pad_sequences(
  tokenizer.texts_to_sequences(X_test),  maxlen=MAX_LEN, padding='post', truncating='post'
)

# Build model
EMBED_DIM = 128
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(MAX_VOCAB, EMBED_DIM),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Train
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    patience=3, 
    restore_best_weights=True
)
history = model.fit(
    X_train_pad, y_train,
    validation_split=0.2,
    epochs=10,
    batch_size=32,
    class_weight=class_weights,
    callbacks=[early_stop]
)

# Evaluate
loss, acc = model.evaluate(X_test_pad, y_test, verbose=0)
print(f"Test Accuracy: {acc*100:.2f}%")